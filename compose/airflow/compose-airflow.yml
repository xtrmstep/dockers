version: '3'
services:

  airflow-server:
    container_name: airflow-server
    image: ${IMAGE_AIRFLOW}
    restart: always
    environment: 
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow_user:12345@postgre:5432/airflow
      - AIRFLOW__SMTP__SMTP_HOST=smtp.office365.com
      - AIRFLOW__SMTP__SMTP_USER=<email@domain.com>  # should be changed to a valid one
      - AIRFLOW__SMTP__SMTP_PASSWORD=<password>  # should be changed to a valid one
      - AIRFLOW__SMTP__SMTP_PORT=587
      - AIRFLOW__SMTP__SMTP_MAIL_FROM=<email@domain.com>  # should be changed to a valid one
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8400  # should be changed to a valid one
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=10
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True
    volumes:
      # set in environment variables:
      # - LOCAL_DOCKER_VOLUMES - path to the volume with Airflow (if you have different location on dev machines)
      # - PYTHONPATH - path to your common packages
      # Examples:
      # - "c:/your_project/dags:/opt/airflow/dags/your_project"
      - "${PYTHONPATH}/packages/:/opt/airflow/dags/packages"
      - "${LOCAL_DOCKER_VOLUMES}${DATA_AIRFLOW_LOGS}:/opt/airflow/logs"
      - /var/run/docker.sock:/var/run/docker.sock # required for Docker operator
      # use this if you need to access some files inside Airflow DAG
      # for example, variable could point to a file in this folder
      - "${LOCAL_DOCKER_VOLUMES}${DATA_AIRFLOW_EXTERNAL}:/external_files"
    ports:
      - "8400:8080"
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3

networks:
  default:
    external:
      name: local-sandbox-networks
